# Aula 1: InstalaÃ§Ã£o Local do Apache Spark
Este guia cobre a instalaÃ§Ã£o do Apache Spark localmente em Windows, macOS e Linux. Siga os passos conforme o sistema operacional que vocÃª utiliza.

## PrÃ©-requisitos
  - Java 8 ou 11: O Spark precisa do Java instalado.
  - Python 3.6 ou superior: Para compatibilidade com o PySpark.
  - Terminal: Prompt de Comando (Windows) ou Terminal (macOS/Linux).

## ðŸªŸ - Windows
#### Passo 1: Instalar o Java
1- Baixe o OpenJDK 11 do site do AdoptOpenJDK (.msi installer).

2- Execute o instalador.

3- Configure a variÃ¡vel de ambiente `JAVA_HOME`:
  - Abra â€œEditar variÃ¡veis de ambiente do sistemaâ€ no Menu Iniciar.
  - Adicione uma nova variÃ¡vel de sistema:
    - Nome: `JAVA_HOME`
    - Valor: `C:\Program Files\AdoptOpenJDK\jdk-11.0.11.9-hotspot` (ajuste o caminho conforme necessÃ¡rio).
  - Edite a variÃ¡vel `Path` e adicione: `%JAVA_HOME%\bin`
    
4- Verifique: Abra um novo Prompt de Comando e execute `java -version`.

#### Passo 2: Instalar o Python
1- Baixe o Python 3.6+ em [python.org](python.org).

2- Execute o instalador e marque a opÃ§Ã£o **â€œAdd Python to PATHâ€**.

3- Verifique: Execute `python --version` em um novo Prompt de Comando.

#### Passo 3: Instalar o Spark
1- Baixe o Spark 3.5.5 (Hadoop 3.x) de [spark.apache.org](spark.apache.org) (arquivo `.tgz`).

2- Extraia o conteÃºdo para `C:\spark-3.5.5-bin-hadoop3` usando o 7-Zip.

3- Configure a variÃ¡vel de ambiente `SPARK_HOME`:
  - Nome: `SPARK_HOME`
  - Valor: `C:\spark-3.5.5-bin-hadoop3`
  - Edite o `Path` e adicione: `%SPARK_HOME%\bin`

4- Instale o `winutils`:
  - Baixe o `winutils.exe` compatÃ­vel com Hadoop 3.x [neste repositÃ³rio GitHub](https://github.com/steveloughran/winutils).
  - Coloque o arquivo em `C:\hadoop\bin`.
  - Adicione a variÃ¡vel:
    - Nome: `HADOOP_HOME`
    - Valor: `C:\hadoop`
    - Adicione `%HADOOP_HOME%\bin` ao `Path`.

#### Passo 4: Verificar
Abra um novo Prompt de Comando e execute:
````bash
spark-shell
````
VocÃª deve ver o logo do Spark. Para sair, digite `:q`.









## ðŸŽ - MacOS
#### Passo 1: Instalar o Java
1. Instale o Homebrew (caso ainda nÃ£o tenha):
````bash
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
````

2. Execute:
````bash
brew install openjdk@11
````

3- Configure o `JAVA_HOME`:

Adicione no seu `~/.zshrc` ou `~/.bash_profile`:
````bash
export JAVA_HOME=$(/usr/libexec/java_home -v 11)
````
Depois, execute:
````bash
source ~/.zshrc
````
Verifique com:
````bash
java -version
````

#### Passo 2: Instalar o Python
````bash
brew install python
````

Verifique com:
````bash
python3 --version
````

#### Passo 3: Instalar o Spark
OpÃ§Ã£o 1 â€“ via Homebrew:
````bash
brew install apache-spark
````
(OpÃ§Ã£o rÃ¡pida que instala a versÃ£o mais recente)

OpÃ§Ã£o 2 â€“ Manual:

1- Baixe o Spark 3.5.5 (Hadoop 3.x) de spark.apache.org.

2- Extraia:
````bash
tar -xzf spark-3.5.5-bin-hadoop3.tgz
````

3- Mova para o diretÃ³rio:
````bash
sudo mv spark-3.5.5-bin-hadoop3 /usr/local/spark-3.5.5-bin-hadoop3
````

4- Adicione no `~/.zshrc`:
````bash
export SPARK_HOME=/usr/local/spark-3.5.5-bin-hadoop3
export PATH=$SPARK_HOME/bin:$PATH
````

5- Execute:
````bash
source ~/.zshrc
````

#### Passo 4: Verificar
Execute no terminal:
````bash
spark-shell
````

Procure pelo logo do Spark. Saia com `:q`.












## ðŸ§ - Linux (Ubuntu / Debian-based)
#### Passo 1: Instalar o Java
````bash
sudo apt update
sudo apt install openjdk-11-jdk
````

Configure o `JAVA_HOME`:

Adicione ao `~/.bashrc`:
````bash
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
export PATH=$JAVA_HOME/bin:$PATH
````

Depois, execute:
````bash
source ~/.bashrc
````

Verifique com:
````bash
java -version
````

#### Passo 2: Instalar o Python
````bash
sudo apt install python3 python3-pip
````

Verifique:
````bash
python3 --version
````

#### Passo 3: Instalar o Spark
````bash
wget https://downloads.apache.org/spark/spark-3.5.5/spark-3.5.5-bin-hadoop3.tgz
tar -xzf spark-3.5.5-bin-hadoop3.tgz
sudo mv spark-3.5.5-bin-hadoop3 /opt/spark
````

Configure as variÃ¡veis de ambiente:

Adicione ao `~/.bashrc`:
````bash
export SPARK_HOME=/opt/spark
export PATH=$SPARK_HOME/bin:$PATH
````

Depois, execute:
````bash
source ~/.bashrc
````

#### Passo 4: Verificar
Execute:
````bash
spark-shell
````
O logo do Spark deve aparecer e o prompt `scala>` deve ficar disponÃ­vel. Para sair, digite `:q`.

````
willdeglan@SQLDicas:~$ spark-shell
25/05/05 22:58:41 WARN Utils: Your hostname, Latitude3410 resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/05/05 22:58:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/05/05 22:58:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Spark context Web UI available at http://10.255.255.254:4040
Spark context available as 'sc' (master = local[*], app id = local-1746496729372).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.5.5
      /_/
         
Using Scala version 2.12.18 (OpenJDK 64-Bit Server VM, Java 11.0.26)
Type in expressions to have them evaluated.
Type :help for more information.

scala>
````

Execute:
````bash
pyspark
````

O logo do Spark deve aparecer e o prompt `spark (>>>)` deve ficar disponÃ­vel. Para sair, digite `:q`.
````
willdeglan@SQLDicas:~$ pyspark
Python 3.12.3 (main, Feb  4 2025, 14:48:35) [GCC 13.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
25/05/05 23:01:53 WARN Utils: Your hostname, Latitude3410 resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/05/05 23:01:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/05/05 23:01:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.5.5
      /_/

Using Python version 3.12.3 (main, Feb  4 2025 14:48:35)
Spark context Web UI available at http://10.255.255.254:4040
Spark context available as 'sc' (master = local[*], app id = local-1746496915005).
SparkSession available as 'spark'.
>>> 
````












